---
title: "Chapter 16: Multimodal AI"
sidebar_label: "Chapter 16: Multimodal AI"
sidebar_position: 16
---

# Chapter 16: Multimodal AI

## Integrating Vision, Language, and Speech for Humanoid Robotics

Welcome to Chapter 16! This chapter explores the cutting-edge field of multimodal artificial intelligence, where machines learn to understand and integrate information from multiple sensory modalities just like humans do. For humanoid robotics, multimodal AI is essential for creating truly intelligent systems that can perceive, understand, and interact with the world in natural ways.

## ðŸŽ¯ Chapter Overview

### Learning Objectives
By the end of this chapter, you will be able to:
- Design multimodal transformer architectures for robotics
- Implement cross-modal attention mechanisms
- Create vision-language models for scene understanding
- Develop audio-visual fusion systems
- Apply multimodal learning to real robot applications
- Understand the challenges and solutions in multimodal AI

### Prerequisites
- **Chapter 13**: Perception Algorithms
- **Chapter 15**: Edge Deployment
- Deep learning fundamentals
- Transformer architecture knowledge
- Python programming expertise

## ðŸ§  Multimodal AI Fundamentals

### What is Multimodal AI?

Multimodal AI refers to artificial intelligence systems that can process, understand, and generate information across multiple modalities such as:

- **Vision**: Images, videos, depth maps
- **Language**: Text, speech, commands
- **Audio**: Sound, music, environmental audio
- **Sensor Data**: LiDAR, IMU, tactile information
- **Actions**: Robot movements, gestures

#### **Key Challenges**

1. **Representation Alignment**: Different modalities have different data structures
2. **Fusion Strategies**: How to combine information effectively
3. **Missing Modalities**: Handle cases where some data is unavailable
4. **Computational Efficiency**: Real-time processing constraints
5. **Cross-Modal Transfer**: Learning from one modality to another

### Multimodal Architectures

#### **Early Fusion**

```python
class EarlyFusionModel(nn.Module):
    def __init__(self, vision_dim, audio_dim, text_dim, hidden_dim):
        super().__init__()

        # Feature extractors for each modality
        self.vision_encoder = VisionEncoder(vision_dim, hidden_dim)
        self.audio_encoder = AudioEncoder(audio_dim, hidden_dim)
        self.text_encoder = TextEncoder(text_dim, hidden_dim)

        # Fusion layer (concatenation)
        fusion_input_dim = hidden_dim * 3
        self.fusion_layer = nn.Sequential(
            nn.Linear(fusion_input_dim, hidden_dim * 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim * 2, hidden_dim)
        )

        # Output heads for different tasks
        self.classification_head = nn.Linear(hidden_dim, num_classes)
        self.regression_head = nn.Linear(hidden_dim, output_dim)

    def forward(self, vision_input, audio_input, text_input):
        # Extract features from each modality
        vision_features = self.vision_encoder(vision_input)
        audio_features = self.audio_encoder(audio_input)
        text_features = self.text_encoder(text_input)

        # Early fusion - concatenate features
        combined_features = torch.cat([
            vision_features, audio_features, text_features
        ], dim=-1)

        # Process fused representation
        fused_representation = self.fusion_layer(combined_features)

        # Generate outputs
        classification_output = self.classification_head(fused_representation)
        regression_output = self.regression_head(fused_representation)

        return {
            'classification': classification_output,
            'regression': regression_output,
            'features': fused_representation
        }
```

#### **Late Fusion**

```python
class LateFusionModel(nn.Module):
    def __init__(self, vision_dim, audio_dim, text_dim, output_dim):
        super().__init__()

        # Modality-specific models
        self.vision_model = VisionClassifier(vision_dim, output_dim)
        self.audio_model = AudioClassifier(audio_dim, output_dim)
        self.text_model = TextClassifier(text_dim, output_dim)

        # Fusion weights (learnable)
        self.fusion_weights = nn.Parameter(torch.ones(3) / 3)

        # Optional fusion network
        self.fusion_network = nn.Sequential(
            nn.Linear(output_dim, output_dim // 2),
            nn.ReLU(),
            nn.Linear(output_dim // 2, output_dim)
        )

    def forward(self, vision_input, audio_input, text_input):
        # Process each modality independently
        vision_output = self.vision_model(vision_input)
        audio_output = self.audio_model(audio_input)
        text_output = self.text_model(text_input)

        # Normalize fusion weights
        weights = F.softmax(self.fusion_weights, dim=0)

        # Weighted combination of outputs
        fused_output = (
            weights[0] * vision_output +
            weights[1] * audio_output +
            weights[2] * text_output
        )

        # Optional: further processing
        final_output = self.fusion_network(fused_output)

        return {
            'fused_output': final_output,
            'modality_outputs': {
                'vision': vision_output,
                'audio': audio_output,
                'text': text_output
            },
            'fusion_weights': weights
        }
```

#### **Cross-Modal Attention**

```python
class CrossModalAttention(nn.Module):
    def __init__(self, vision_dim, text_dim, num_heads=8):
        super().__init__()
        self.num_heads = num_heads
        self.vision_dim = vision_dim
        self.text_dim = text_dim

        # Linear projections for attention
        self.vision_proj = nn.Linear(vision_dim, vision_dim)
        self.text_proj = nn.Linear(text_dim, vision_dim)

        # Multi-head attention layers
        self.vision_to_text_attention = nn.MultiheadAttention(
            embed_dim=vision_dim, num_heads=num_heads, batch_first=True
        )
        self.text_to_vision_attention = nn.MultiheadAttention(
            embed_dim=vision_dim, num_heads=num_heads, batch_first=True
        )

        # Output projections
        self.vision_output = nn.Linear(vision_dim, vision_dim)
        self.text_output = nn.Linear(vision_dim, text_dim)

        # Layer normalization
        self.vision_norm = nn.LayerNorm(vision_dim)
        self.text_norm = nn.LayerNorm(vision_dim)

    def forward(self, vision_features, text_features):
        # Project features to common dimension
        vision_proj = self.vision_proj(vision_features)
        text_proj = self.text_proj(text_features)

        # Cross-modal attention: vision attends to text
        vision_attended, _ = self.vision_to_text_attention(
            vision_proj, text_proj, text_proj
        )
        vision_attended = self.vision_norm(vision_proj + vision_attended)

        # Cross-modal attention: text attends to vision
        text_attended, _ = self.text_to_vision_attention(
            text_proj, vision_proj, vision_proj
        )
        text_attended = self.text_norm(text_proj + text_attended)

        # Output projections
        vision_output = self.vision_output(vision_attended)
        text_output = self.text_output(text_attended)

        return vision_output, text_output

class MultimodalTransformer(nn.Module):
    def __init__(self, config):
        super().__init__()

        # Modality encoders
        self.vision_encoder = VisionEncoder(config.vision_config)
        self.audio_encoder = AudioEncoder(config.audio_config)
        self.text_encoder = TextEncoder(config.text_config)

        # Cross-modal attention layers
        self.vision_text_attention = CrossModalAttention(
            config.vision_dim, config.text_dim, config.num_heads
        )
        self.audio_vision_attention = CrossModalAttention(
            config.audio_dim, config.vision_dim, config.num_heads
        )

        # Fusion transformer
        self.fusion_transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(
                d_model=config.fusion_dim,
                nhead=config.num_heads,
                dim_feedforward=config.ff_dim,
                dropout=config.dropout
            ),
            num_layers=config.num_fusion_layers
        )

        # Output heads
        self.output_heads = nn.ModuleDict({
            'classification': nn.Linear(config.fusion_dim, config.num_classes),
            'regression': nn.Linear(config.fusion_dim, config.output_dim),
            'generation': nn.Linear(config.fusion_dim, config.vocab_size)
        })

    def forward(self, inputs):
        # Extract modality features
        vision_features = self.vision_encoder(inputs['vision'])
        audio_features = self.audio_encoder(inputs['audio'])
        text_features = self.text_encoder(inputs['text'])

        # Cross-modal attention
        vision_attended, text_attended = self.vision_text_attention(
            vision_features, text_features
        )

        # Create fused sequence
        fused_sequence = torch.cat([
            vision_attended,
            audio_features,
            text_attended
        ], dim=1)

        # Apply fusion transformer
        fused_representation = self.fusion_transformer(fused_sequence)

        # Generate outputs
        outputs = {}
        for task_name, head in self.output_heads.items():
            # Use mean pooling for classification/regression
            pooled = fused_representation.mean(dim=1)
            outputs[task_name] = head(pooled)

        return outputs
```

## ðŸ¤– Vision-Language Models for Robotics

### CLIP-inspired Models

#### **Contrastive Learning for Vision-Language**

```python
class VisionLanguageContrastive(nn.Module):
    def __init__(self, vision_config, text_config, embedding_dim=512):
        super().__init__()

        # Vision encoder (ResNet or ViT)
        self.vision_encoder = VisionEncoder(vision_config)
        self.vision_projection = nn.Linear(
            vision_config.hidden_size, embedding_dim
        )

        # Text encoder (Transformer)
        self.text_encoder = TextEncoder(text_config)
        self.text_projection = nn.Linear(
            text_config.hidden_size, embedding_dim
        )

        # Temperature parameter for contrastive learning
        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))

        # Robot-specific projection
        self.robot_projection = nn.Linear(embedding_dim, embedding_dim)

    def encode_image(self, images):
        # Encode images and project to embedding space
        vision_features = self.vision_encoder(images)
        image_embeddings = self.vision_projection(vision_features)

        # Normalize embeddings
        image_embeddings = F.normalize(image_embeddings, dim=-1)
        return image_embeddings

    def encode_text(self, texts):
        # Encode text and project to embedding space
        text_features = self.text_encoder(texts)
        text_embeddings = self.text_projection(text_features)

        # Normalize embeddings
        text_embeddings = F.normalize(text_embeddings, dim=-1)
        return text_embeddings

    def forward(self, images, texts):
        image_embeddings = self.encode_image(images)
        text_embeddings = self.encode_text(texts)

        # Calculate similarity scores
        logit_scale = self.logit_scale.exp()
        similarity_matrix = torch.matmul(
            image_embeddings, text_embeddings.t()
        ) * logit_scale

        # Project for robot actions
        robot_embeddings = self.robot_projection(image_embeddings)

        return {
            'similarity_matrix': similarity_matrix,
            'image_embeddings': image_embeddings,
            'text_embeddings': text_embeddings,
            'robot_embeddings': robot_embeddings
        }

class RobotCLIP(nn.Module):
    def __init__(self, clip_model, action_dim=7):
        super().__init__()
        self.clip_model = clip_model

        # Action prediction head
        self.action_head = nn.Sequential(
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, action_dim)
        )

        # Scene understanding head
        self.scene_head = nn.Sequential(
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 128),  # Scene embedding dimension
            nn.ReLU(),
            nn.Linear(128, 50)    # Number of scene classes
        )

    def forward(self, images, text_commands):
        # Get CLIP embeddings
        clip_outputs = self.clip_model(images, text_commands)
        image_embeddings = clip_outputs['image_embeddings']

        # Predict robot actions based on visual understanding
        actions = self.action_head(image_embeddings)

        # Understand scene context
        scene_predictions = self.scene_head(image_embeddings)

        return {
            'actions': actions,
            'scene_understanding': scene_predictions,
            'similarity_matrix': clip_outputs['similarity_matrix']
        }
```

#### **Visual Question Answering for Robots**

```python
class VQAModel(nn.Module):
    def __init__(self, vision_config, text_config, vocab_size):
        super().__init__()

        # Vision encoder
        self.vision_encoder = VisionEncoder(vision_config)

        # Text encoder
        self.text_encoder = TextEncoder(text_config)

        # Multimodal fusion
        self.fusion_layer = nn.MultiheadAttention(
            embed_dim=vision_config.hidden_size,
            num_heads=8,
            batch_first=True
        )

        # Answer generation
        self.answer_generator = nn.TransformerDecoder(
            nn.TransformerDecoderLayer(
                d_model=vision_config.hidden_size,
                nhead=8,
                dim_feedforward=2048,
                dropout=0.1,
                batch_first=True
            ),
            num_layers=6
        )

        # Output vocabulary
        self.vocab_projection = nn.Linear(
            vision_config.hidden_size, vocab_size
        )

    def forward(self, images, questions, max_length=20):
        # Encode images
        vision_features = self.vision_encoder(images)

        # Encode questions
        question_features = self.text_encoder(questions)

        # Fuse vision and question information
        fused_features, _ = self.fusion_layer(
            vision_features, question_features, question_features
        )

        # Generate answers autoregressively
        batch_size = images.size(0)
        device = images.device

        # Start with SOS token
        answer_input = torch.zeros(
            batch_size, 1, dtype=torch.long, device=device
        )

        generated_answers = []

        for _ in range(max_length):
            # Generate next token
            answer_features = self.text_encoder.embed_tokens(answer_input)

            # Decode using fused vision-question features
            decoded_features = self.answer_generator(
                answer_features, fused_features
            )

            # Project to vocabulary
            logits = self.vocab_projection(decoded_features)

            # Get next token (greedy)
            next_token = torch.argmax(logits[:, -1:, :], dim=-1)
            generated_answers.append(next_token)

            # Append to input for next iteration
            answer_input = torch.cat([answer_input, next_token], dim=1)

            # Stop if all sequences generated EOS
            if (next_token == self.eos_token_id).all():
                break

        return torch.cat(generated_answers, dim=1)

class RobotVQA(nn.Module):
    def __init__(self, vqa_model, num_actions):
        super().__init__()
        self.vqa_model = vqa_model

        # Action selection from VQA output
        self.action_selector = nn.Sequential(
            nn.Linear(vqa_model.hidden_size, 256),
            nn.ReLU(),
            nn.Linear(256, num_actions)
        )

        # Confidence estimator
        self.confidence_estimator = nn.Sequential(
            nn.Linear(vqa_model.hidden_size, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )

    def answer_and_act(self, images, questions):
        # Get VQA answers
        answers = self.vqa_model(images, questions)

        # Extract fused features for action selection
        vision_features = self.vqa_model.vision_encoder(images)
        question_features = self.vqa_model.text_encoder(questions)
        fused_features, _ = self.vqa_model.fusion_layer(
            vision_features, question_features, question_features
        )

        # Select actions based on understanding
        pooled_features = fused_features.mean(dim=1)
        actions = self.action_selector(pooled_features)
        confidence = self.confidence_estimator(pooled_features)

        return {
            'answers': answers,
            'actions': actions,
            'confidence': confidence
        }
```

## ðŸ”Š Audio-Visual Integration

### Audio-Visual Fusion

#### **Audio-Visual Attention**

```python
class AudioVisualAttention(nn.Module):
    def __init__(self, audio_dim, visual_dim, hidden_dim=512):
        super().__init__()

        # Projections to common space
        self.audio_proj = nn.Linear(audio_dim, hidden_dim)
        self.visual_proj = nn.Linear(visual_dim, hidden_dim)

        # Cross-modal attention
        self.audio_to_visual = nn.MultiheadAttention(
            hidden_dim, num_heads=8, batch_first=True
        )
        self.visual_to_audio = nn.MultiheadAttention(
            hidden_dim, num_heads=8, batch_first=True
        )

        # Fusion layer
        self.fusion_layer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(
                d_model=hidden_dim,
                nhead=8,
                dim_feedforward=hidden_dim * 4,
                dropout=0.1,
                batch_first=True
            ),
            num_layers=4
        )

        # Output projections
        self.audio_output = nn.Linear(hidden_dim, audio_dim)
        self.visual_output = nn.Linear(hidden_dim, visual_dim)

    def forward(self, audio_features, visual_features):
        # Project to common dimension
        audio_proj = self.audio_proj(audio_features)
        visual_proj = self.visual_proj(visual_features)

        # Cross-modal attention
        audio_attended, _ = self.audio_to_visual(
            audio_proj, visual_proj, visual_proj
        )
        visual_attended, _ = self.visual_to_audio(
            visual_proj, audio_proj, audio_proj
        )

        # Combine attended features
        combined_features = torch.cat([audio_attended, visual_attended], dim=1)

        # Apply fusion transformer
        fused_features = self.fusion_transformer(combined_features)

        # Split and project back
        fused_audio = fused_features[:, :audio_attended.size(1), :]
        fused_visual = fused_features[:, audio_attended.size(1):, :]

        audio_output = self.audio_output(fused_audio)
        visual_output = self.visual_output(fused_visual)

        return audio_output, visual_output, fused_features

class AudioVisualRobot(nn.Module):
    def __init__(self, config):
        super().__init__()

        # Audio encoder for speech and environmental sounds
        self.audio_encoder = AudioEncoder(config.audio_config)

        # Visual encoder for camera input
        self.visual_encoder = VisualEncoder(config.visual_config)

        # Audio-visual fusion
        self.av_attention = AudioVisualAttention(
            config.audio_dim, config.visual_dim, config.hidden_dim
        )

        # Robot control heads
        self.action_head = nn.Linear(config.hidden_dim, config.action_dim)
        self.speech_head = nn.Linear(config.hidden_dim, config.vocab_size)

        # Sound source localization
        self.localization_head = nn.Sequential(
            nn.Linear(config.hidden_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 3)  # x, y, z coordinates
        )

    def forward(self, audio_input, visual_input):
        # Encode modalities
        audio_features = self.audio_encoder(audio_input)
        visual_features = self.visual_encoder(visual_input)

        # Fuse audio-visual information
        audio_fused, visual_fused, av_features = self.av_attention(
            audio_features, visual_features
        )

        # Generate robot outputs
        pooled_features = av_features.mean(dim=1)

        actions = self.action_head(pooled_features)
        speech_output = self.speech_head(pooled_features)
        sound_location = self.localization_head(pooled_features)

        return {
            'actions': actions,
            'speech_output': speech_output,
            'sound_location': sound_location,
            'audio_features': audio_fused,
            'visual_features': visual_fused
        }
```

#### **Speech-Gesture Synchronization**

```python
class SpeechGestureSync(nn.Module):
    def __init__(self, speech_dim, gesture_dim, sync_dim=256):
        super().__init__()

        # Speech encoder
        self.speech_encoder = SpeechEncoder(speech_dim)

        # Gesture encoder
        self.gesture_encoder = GestureEncoder(gesture_dim)

        # Synchronization module
        self.sync_encoder = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(
                d_model=sync_dim,
                nhead=8,
                dim_feedforward=sync_dim * 4,
                batch_first=True
            ),
            num_layers=6
        )

        # Projections to synchronization space
        self.speech_proj = nn.Linear(speech_dim, sync_dim)
        self.gesture_proj = nn.Linear(gesture_dim, sync_dim)

        # Gesture generation from speech
        self.gesture_generator = nn.TransformerDecoder(
            nn.TransformerDecoderLayer(
                d_model=sync_dim,
                nhead=8,
                dim_feedforward=sync_dim * 4,
                batch_first=True
            ),
            num_layers=4
        )

        # Temporal alignment loss
        self.temporal_aligner = TemporalAlignmentLayer(sync_dim)

    def forward(self, speech_input, gesture_input=None):
        # Encode speech
        speech_features = self.speech_encoder(speech_input)
        speech_proj = self.speech_proj(speech_features)

        if gesture_input is not None:
            # Encode gestures for training
            gesture_features = self.gesture_encoder(gesture_input)
            gesture_proj = self.gesture_proj(gesture_features)

            # Synchronize speech and gestures
            combined_input = torch.cat([speech_proj, gesture_proj], dim=1)
            synced_features = self.sync_encoder(combined_input)

            # Split synchronized features
            speech_sync = synced_features[:, :speech_proj.size(1), :]
            gesture_sync = synced_features[:, speech_proj.size(1):, :]

            return speech_sync, gesture_sync
        else:
            # Generate gestures from speech (inference)
            generated_gestures = self.gesture_generator(
                speech_proj, speech_proj
            )

            return speech_proj, generated_gestures

class TemporalAlignmentLayer(nn.Module):
    def __init__(self, hidden_dim):
        super().__init__()
        self.hidden_dim = hidden_dim

        # Temporal attention for alignment
        self.temporal_attention = nn.MultiheadAttention(
            hidden_dim, num_heads=8, batch_first=True
        )

        # Alignment loss computation
        self.alignment_head = nn.Linear(hidden_dim, 1)

    def forward(self, speech_features, gesture_features):
        # Compute alignment scores
        speech_expanded = speech_features.unsqueeze(2)  # [B, T_s, 1, D]
        gesture_expanded = gesture_features.unsqueeze(1)  # [B, 1, T_g, D]

        # Compute similarity matrix
        similarity = torch.matmul(
            speech_expanded, gesture_expanded.transpose(-2, -1)
        ).squeeze(-1)  # [B, T_s, T_g]

        # Apply softmax to get alignment probabilities
        alignment_probs = F.softmax(similarity, dim=-1)

        # Apply attention
        aligned_gestures = torch.matmul(
            alignment_probs, gesture_features
        )  # [B, T_s, D]

        return aligned_gestures, alignment_probs
```

## ðŸŽ¯ Practical Implementation

### Multimodal Robot Controller

```python
class MultimodalRobotController:
    def __init__(self, model_path, device='cuda'):
        self.device = device
        self.model = self.load_model(model_path, device)

        # Modality processors
        self.vision_processor = VisionProcessor()
        self.audio_processor = AudioProcessor()
        self.text_processor = TextProcessor()

        # Robot interfaces
        self.robot_interface = RobotInterface()
        self.speech_interface = SpeechInterface()

    def load_model(self, model_path, device):
        """Load pretrained multimodal model"""
        model = torch.load(model_path, map_location=device)
        model.eval()
        return model.to(device)

    def process_multimodal_input(self, vision_data=None, audio_data=None, text_data=None):
        """Process multimodal input and generate robot responses"""

        # Prepare input dictionary
        model_input = {}

        if vision_data is not None:
            model_input['vision'] = self.vision_processor.process(vision_data)

        if audio_data is not None:
            model_input['audio'] = self.audio_processor.process(audio_data)

        if text_data is not None:
            model_input['text'] = self.text_processor.process(text_data)

        # Run inference
        with torch.no_grad():
            outputs = self.model(model_input)

        return outputs

    def execute_robot_behavior(self, outputs):
        """Execute robot actions based on multimodal understanding"""

        # Extract actions
        if 'actions' in outputs:
            actions = outputs['actions'].cpu().numpy()
            self.robot_interface.execute_actions(actions)

        # Generate speech response
        if 'speech_output' in outputs:
            speech_tokens = outputs['speech_output']
            speech_text = self.text_processor.tokens_to_text(speech_tokens)
            self.speech_interface.speak(speech_text)

        # Handle sound localization
        if 'sound_location' in outputs:
            sound_location = outputs['sound_location'].cpu().numpy()
            self.robot_interface.turn_towards_sound(sound_location)

    def continuous_interaction_loop(self):
        """Main interaction loop for continuous human-robot interaction"""

        print("Starting multimodal robot interaction...")

        while True:
            try:
                # Collect multimodal input
                vision_data = self.vision_processor.capture_camera()
                audio_data = self.audio_processor.capture_microphone()

                # Check for voice commands
                if self.audio_processor.detect_speech(audio_data):
                    text_command = self.audio_processor.speech_to_text(audio_data)
                else:
                    text_command = None

                # Process multimodal input
                outputs = self.process_multimodal_input(
                    vision_data=vision_data,
                    audio_data=audio_data,
                    text_data=text_command
                )

                # Execute robot behavior
                self.execute_robot_behavior(outputs)

                # Small delay to prevent overwhelming the system
                time.sleep(0.1)

            except KeyboardInterrupt:
                print("Stopping robot interaction...")
                break
            except Exception as e:
                print(f"Error in interaction loop: {e}")
                continue

# ROS 2 Integration
class MultimodalRobotNode(Node):
    def __init__(self):
        super().__init__('multimodal_robot_node')

        # Initialize multimodal controller
        self.controller = MultimodalRobotController(
            model_path='/models/multimodal_robot.pth'
        )

        # ROS 2 subscribers
        self.image_sub = self.create_subscription(
            Image, '/camera/image_raw', self.image_callback, 10
        )
        self.audio_sub = self.create_subscription(
            AudioData, '/microphone/audio_raw', self.audio_callback, 10
        )

        # ROS 2 publishers
        self.action_pub = self.create_publisher(
            JointCommand, '/robot/joint_commands', 10
        )
        self.speech_pub = self.create_publisher(
            String, '/robot/speech_output', 10
        )

        # State tracking
        self.latest_image = None
        self.latest_audio = None

        self.get_logger().info('Multimodal Robot Node initialized')

    def image_callback(self, msg):
        """Handle camera image input"""
        self.latest_image = msg

        # Process with multimodal model
        if self.latest_audio is not None:
            self.process_multimodal_input()

    def audio_callback(self, msg):
        """Handle microphone audio input"""
        self.latest_audio = msg

        # Process with multimodal model
        if self.latest_image is not None:
            self.process_multimodal_input()

    def process_multimodal_input(self):
        """Process current multimodal input and generate outputs"""

        try:
            # Convert ROS messages to model input format
            vision_data = self.ros_image_to_tensor(self.latest_image)
            audio_data = self.ros_audio_to_tensor(self.latest_audio)

            # Get model outputs
            outputs = self.controller.process_multimodal_input(
                vision_data=vision_data,
                audio_data=audio_data
            )

            # Publish ROS messages
            self.publish_robot_outputs(outputs)

        except Exception as e:
            self.get_logger().error(f"Error processing multimodal input: {e}")

    def publish_robot_outputs(self, outputs):
        """Publish robot actions and speech as ROS messages"""

        # Publish joint commands
        if 'actions' in outputs:
            joint_cmd = JointCommand()
            joint_cmd.joint_names = ['joint_1', 'joint_2', 'joint_3']
            joint_cmd.positions = outputs['actions'].cpu().numpy().tolist()
            self.action_pub.publish(joint_cmd)

        # Publish speech output
        if 'speech_output' in outputs:
            speech_msg = String()
            speech_text = self.controller.text_processor.tokens_to_text(
                outputs['speech_output']
            )
            speech_msg.data = speech_text
            self.speech_pub.publish(speech_msg)

def main():
    rclpy.init()

    try:
        node = MultimodalRobotNode()
        rclpy.spin(node)
    except KeyboardInterrupt:
        pass
    finally:
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## ðŸ“Š Training and Evaluation

### Multimodal Dataset Handling

```python
class MultimodalDataset(Dataset):
    def __init__(self, vision_root, audio_root, text_file, transform=None):
        self.vision_root = vision_root
        self.audio_root = audio_root
        self.transform = transform

        # Load text annotations
        with open(text_file, 'r') as f:
            self.annotations = json.load(f)

        # Data modalities for each sample
        self.samples = self.prepare_samples()

    def prepare_samples(self):
        samples = []

        for annotation in self.annotations:
            sample = {
                'id': annotation['id'],
                'vision_path': os.path.join(
                    self.vision_root, annotation['vision_file']
                ),
                'audio_path': os.path.join(
                    self.audio_root, annotation['audio_file']
                ),
                'text': annotation['text'],
                'labels': annotation.get('labels', {}),
                'missing_modalities': annotation.get('missing_modalities', [])
            }
            samples.append(sample)

        return samples

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        sample = self.samples[idx]

        # Load vision data (if available)
        if 'vision' not in sample['missing_modalities']:
            vision_data = self.load_vision_data(sample['vision_path'])
            if self.transform:
                vision_data = self.transform(vision_data)
        else:
            vision_data = None

        # Load audio data (if available)
        if 'audio' not in sample['missing_modalities']:
            audio_data = self.load_audio_data(sample['audio_path'])
        else:
            audio_data = None

        # Prepare text data
        text_data = self.process_text(sample['text'])

        return {
            'vision': vision_data,
            'audio': audio_data,
            'text': text_data,
            'labels': sample['labels'],
            'id': sample['id']
        }

    def load_vision_data(self, path):
        """Load and preprocess vision data"""
        image = Image.open(path).convert('RGB')
        return image

    def load_audio_data(self, path):
        """Load and preprocess audio data"""
        waveform, sample_rate = torchaudio.load(path)

        # Resample if necessary
        if sample_rate != 16000:
            resampler = torchaudio.transforms.Resample(
                sample_rate, 16000
            )
            waveform = resampler(waveform)

        return waveform

    def process_text(self, text):
        """Tokenize and process text"""
        # Implement tokenization logic
        tokens = self.tokenizer.encode(text)
        return tokens

class MultimodalTrainer:
    def __init__(self, model, train_dataset, val_dataset, config):
        self.model = model
        self.train_dataset = train_dataset
        self.val_dataset = val_dataset
        self.config = config

        # Data loaders
        self.train_loader = DataLoader(
            train_dataset, batch_size=config.batch_size,
            shuffle=True, num_workers=4, collate_fn=self.collate_fn
        )
        self.val_loader = DataLoader(
            val_dataset, batch_size=config.batch_size,
            shuffle=False, num_workers=4, collate_fn=self.collate_fn
        )

        # Optimizer and scheduler
        self.optimizer = torch.optim.AdamW(
            model.parameters(), lr=config.learning_rate,
            weight_decay=config.weight_decay
        )

        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, T_max=config.num_epochs
        )

        # Loss functions
        self.criterion = nn.CrossEntropyLoss()
        self.contrastive_criterion = ContrastiveLoss()

    def collate_fn(self, batch):
        """Custom collate function for multimodal data"""
        batch_dict = {
            'vision': [],
            'audio': [],
            'text': [],
            'labels': []
        }

        for item in batch:
            batch_dict['vision'].append(item['vision'])
            batch_dict['audio'].append(item['audio'])
            batch_dict['text'].append(item['text'])
            batch_dict['labels'].append(item['labels'])

        # Handle missing modalities
        processed_batch = {}

        # Process vision data
        vision_batch = [v for v in batch_dict['vision'] if v is not None]
        if vision_batch:
            processed_batch['vision'] = torch.stack(vision_batch)
        else:
            processed_batch['vision'] = None

        # Process audio data
        audio_batch = [a for a in batch_dict['audio'] if a is not None]
        if audio_batch:
            # Pad audio sequences to same length
            processed_batch['audio'] = pad_sequence(audio_batch, batch_first=True)
        else:
            processed_batch['audio'] = None

        # Process text data
        text_batch = [torch.tensor(t) for t in batch_dict['text']]
        processed_batch['text'] = pad_sequence(text_batch, batch_first=True)

        # Process labels
        processed_batch['labels'] = batch_dict['labels']

        return processed_batch

    def train_epoch(self):
        """Train for one epoch"""
        self.model.train()
        total_loss = 0
        num_batches = 0

        for batch in self.train_loader:
            # Move batch to device
            batch = {k: v.to(self.device) if v is not None else v
                    for k, v in batch.items()}

            # Forward pass
            self.optimizer.zero_grad()
            outputs = self.model(batch)

            # Calculate loss
            loss = self.calculate_loss(outputs, batch)

            # Backward pass
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
            self.optimizer.step()

            total_loss += loss.item()
            num_batches += 1

        return total_loss / num_batches

    def validate_epoch(self):
        """Validate for one epoch"""
        self.model.eval()
        total_loss = 0
        num_batches = 0

        with torch.no_grad():
            for batch in self.val_loader:
                # Move batch to device
                batch = {k: v.to(self.device) if v is not None else v
                        for k, v in batch.items()}

                # Forward pass
                outputs = self.model(batch)

                # Calculate loss
                loss = self.calculate_loss(outputs, batch)

                total_loss += loss.item()
                num_batches += 1

        return total_loss / num_batches

    def calculate_loss(self, outputs, batch):
        """Calculate multimodal loss"""
        loss = 0

        # Task-specific losses
        if 'classification' in outputs and 'classification_labels' in batch['labels']:
            class_loss = self.criterion(
                outputs['classification'],
                batch['labels']['classification_labels']
            )
            loss += class_loss

        # Contrastive learning loss for multimodal alignment
        if 'vision_embeddings' in outputs and 'text_embeddings' in outputs:
            contrastive_loss = self.contrastive_criterion(
                outputs['vision_embeddings'],
                outputs['text_embeddings']
            )
            loss += 0.1 * contrastive_loss

        return loss

    def train(self):
        """Main training loop"""
        best_val_loss = float('inf')

        for epoch in range(self.config.num_epochs):
            # Train
            train_loss = self.train_epoch()

            # Validate
            val_loss = self.validate_epoch()

            # Update learning rate
            self.scheduler.step()

            # Save best model
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                torch.save(
                    self.model.state_dict(),
                    'best_multimodal_model.pth'
                )

            print(f"Epoch {epoch+1}/{self.config.num_epochs}")
            print(f"Train Loss: {train_loss:.4f}")
            print(f"Val Loss: {val_loss:.4f}")
            print(f"Learning Rate: {self.scheduler.get_last_lr()[0]:.6f}")
            print("-" * 50)
```

## ðŸ“‹ Chapter Summary

### Key Takeaways

1. **Multimodal Architecture Patterns**
   - Early fusion: Combine raw features from all modalities
   - Late fusion: Process modalities separately then combine predictions
   - Cross-modal attention: Let modalities attend to each other

2. **Vision-Language Models**
   - Contrastive learning (CLIP) for vision-text alignment
   - Visual Question Answering for robot understanding
   - Text-to-action mapping for robot control

3. **Audio-Visual Integration**
   - Speech-gesture synchronization
   - Sound source localization
   - Multimodal attention mechanisms

4. **Training Strategies**
   - Handle missing modalities gracefully
   - Use contrastive learning for modality alignment
   - Implement data augmentation across modalities

### Practical Applications

1. **Human-Robot Interaction**: Natural communication through multiple channels
2. **Scene Understanding**: Comprehensive environment perception
3. **Task Execution**: Translate multimodal commands into robot actions
4. **Adaptive Behavior**: Learn from different types of feedback

### Next Steps

With multimodal AI mastered, you're ready for Chapter 17: **Vision-Language Models**, where we'll dive deeper into state-of-the-art vision-language architectures and their applications in humanoid robotics.

---

**Ready to proceed?** Continue with [Chapter 17: Vision-Language Models](17-vision-language.md) to explore advanced multimodal understanding! ðŸ¤–âœ¨

**Pro Tip**: Multimodal AI represents the future of intelligent systems. The ability to integrate information from multiple sensory channels will be crucial for creating truly intelligent and interactive humanoid robots! ðŸŒŸðŸŽ¯